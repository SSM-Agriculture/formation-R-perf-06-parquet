---
editor: 
  markdown: 
    wrap: 72
---

# Ecrire des fichiers parquet {.backgroundTitre}

## Données peu volumineuses: écrire un seul fichier Parquet (1/2)

<br>

En tant que responsable de sources, vous pouvez être amenés à écrire et déposer des fichiers Parquet, par exemple sous Cerise.  

Pour cela, on utilise la fonction `write_parquet()`.  
Un 1er exemple simple à partir d'un fichier rds:  

```{.r}
# Lecture du fichier rds
msa_ns <- readRDS("data/msa_ns_src_2023.rds")

# Écriture des données en format Parquet
write_parquet(x = msa_ns, sink = "data/msa_ns_src_2023.parquet")
```

<br>

![](img/msa_parquet.png)

## Données peu volumineuses: écrire un seul fichier Parquet (2/2)

<br>

Un autre exemple un peu plus compliqué à partir de fichier csv contenu dans un zip sur internet :

<br>

```{.r}
# Chargement des packages
library(arrow)
library(readr)

# Téléchargement du fichier zip
download.file("https://www.insee.fr/fr/statistiques/fichier/2540004/dpt2021_csv.zip", destfile = "data/dpt2021_csv.zip") 
# Décompression du fichier zip
unzip("data/dpt2021_csv.zip", exdir = "data")

# Lecture du fichier CSV
dpt2021 <- read_delim(file = "data/dpt2021.csv")

# Écriture des données en format Parquet
write_parquet(x = dpt2021, sink = "data/dpt2021.parquet"))
```

## Données volumineuses: écrire un fichier Parquet partitionné (1/2)

<br>

Ça veut dire quoi partitionné ?  


<p style="text-align: center;">[Partitionner un fichier revient à le “découper” selon une clé de partionnement (une ou plusieurs variables)]{.Macaron2}</p>


En pratique, l’ensemble des données sera stockée dans plusieurs fichiers au format Parquet.

Voici par exemple comment se présente un fichier Parquet partitionné selon les régions : 

![](img/fichier_partition.png){fig-align="center"}

## Données volumineuses: écrire un fichier Parquet partitionné (2/2)

**Pour écrire des fichiers Parquet partitionnés**, on utilise la fonction `write_dataset()`.

**Partitionnons** notre fichier issu de la MSA **par type d'exploitation et sexe** :  


```{.r}
write_dataset(
  dataset = msa_ns,
  path = "data/msa_ns",
  partitioning = c("TYPE_EXP","SEXE"), # les variables de partitionnement
  format = "parquet"
)
```

Voici un aperçu de l'arborescence créée (:  

```{.r}
data/msa_ns
├── TYPE_EXP=1
│   ├── SEXE=1
│   │   └── part-0.parquet
│   └── SEXE=2
│       └── part-0.parquet
├── TYPE_EXP=2
│   ├── SEXE=1
│   │   └── part-0.parquet
│   └── SEXE=2
│       └── part-0.parquet
└── TYPE_EXP=3
    ├── SEXE=1
    │   └── part-0.parquet
    └── SEXE=2
        └── part-0.parquet
        
        
```

## Industrialiser la conversion de vos fichiers ?

- Le package R [parquetize](https://ddotta.github.io/parquetize/) permet de faciliter la conversion de données au format Parquet.  

- Plusieurs formats supportés csv, json, rds, fst, SAS, SPSS, Stata, sqlite…  

- Propose des [solutions de contournement](https://ddotta.github.io/parquetize/articles/aa-conversions.html) pour les fichiers très volumineux.

Un exemple issu de la documentation :  

```{.r}
Conversion from a local rds file to a partitioned parquet file :: 12
rds_to_parquet(
path_to_file = system.file("extdata","iris.rds",package = "parquetize"),
path_to_parquet = tempfile(fileext = ".parquet"),
partition = "yes",
partitioning = c("Species")
)

#> Reading data...
#> Writing data...
#> ✔ Data are available in parquet dataset under /tmp/RtmptNiaDm/file1897441ca0c0.parquet
#> Writing data...

#> Reading data...
```

