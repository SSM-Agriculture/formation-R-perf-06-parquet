---
editor: 
  markdown: 
    wrap: 72
---

# Écrire des fichiers parquet {.backgroundTitre}

## Données peu volumineuses: écrire un seul fichier Parquet (1/2)

<br>

En tant que responsable de sources, vous pouvez être amenés à écrire et déposer des fichiers Parquet, par exemple sous Cerise.  

Pour cela, on utilise la fonction `write_parquet()`.  
Un 1er exemple simple à partir d'un fichier rds:  

```{.r}
# Lecture du fichier rds
msa_ns <- readRDS("data/msa_ns_src_2023.rds")

# Écriture des données en format Parquet
write_parquet(x = msa_ns, sink = "data/msa_ns_src_2023.parquet")
```

<br>

![](img/msa_parquet.png)

## Données peu volumineuses: écrire un seul fichier Parquet (2/2)

<br>

Un autre exemple un peu plus compliqué à partir de fichier csv contenu dans un zip sur internet :

<br>

```{.r}
# Chargement des packages
library(arrow)
library(readr)

# Téléchargement du fichier zip
download.file("https://www.insee.fr/fr/statistiques/fichier/2540004/dpt2021_csv.zip", destfile = "data/dpt2021_csv.zip") 
# Décompression du fichier zip
unzip("data/dpt2021_csv.zip", exdir = "data")

# Lecture du fichier CSV
dpt2021 <- read_delim(file = "data/dpt2021.csv")

# Écriture des données en format Parquet
write_parquet(x = dpt2021, sink = "data/dpt2021.parquet"))
```


## Données volumineuses: écrire un fichier Parquet partitionné (1/3)

<br>
**Pourquoi partitionner ?**

::: columns
::: {.column width="45%"}

Par définition, il n'est pas possible de charger seulement quelques lignes d'un fichier Parquet : **on importe nécessairement des colonnes entières**.  

Lorsque le fichier Parquet est partitionné, **arrow est capable de filtrer les lignes à importer à l'aide de clés departitionnement**, ce qui permet d'accélérer l'importation des données.  

**Le partitionnement permet de travailler sur des fichiers Parquet de plus petite taille et donc de consommer moins de mémoire vive.**

:::

::: {.column width="55%"}

![](img/predicate_pushdown.png)

:::

::::


## Données volumineuses: écrire un fichier Parquet partitionné (2/3)

<br>

Ça veut dire quoi partitionné ?  


<p style="text-align: center;">[Partitionner un fichier revient à le “découper” selon une clé de partionnement (une ou plusieurs variables)]{.Macaron2}</p>


En pratique, l'ensemble des données sera stockée dans plusieurs fichiers au format Parquet.

Voici par exemple comment se présente un fichier Parquet partitionné selon les régions : 

![](img/fichier_partition.png){fig-align="center"}

## Données volumineuses: écrire un fichier Parquet partitionné (3/3)

**Pour écrire des fichiers Parquet partitionnés**, on utilise la fonction `write_dataset()`.

**Partitionnons** notre fichier issu de la MSA **par type d'exploitation et sexe** :  


```{.r}
write_dataset(
  dataset = msa_ns,
  path = "data/msa_ns",
  partitioning = c("TYPE_EXP","SEXE"), # les variables de partitionnement
  format = "parquet"
)
```

Voici un aperçu de l'arborescence créée (:  

```{.r}
data/msa_ns
├── TYPE_EXP=1
│   ├── SEXE=1
│   │   └── part-0.parquet
│   └── SEXE=2
│       └── part-0.parquet
├── TYPE_EXP=2
│   ├── SEXE=1
│   │   └── part-0.parquet
│   └── SEXE=2
│       └── part-0.parquet
└── TYPE_EXP=3
    ├── SEXE=1
    │   └── part-0.parquet
    └── SEXE=2
        └── part-0.parquet
        
        
```

## Forcer les types des colonnes lors de l'écriture de fichiers Parquet

<br>

En tant que responsable de sources par exemple, vous pouvez forcer le typage des colonnes d'un fichier Parquet.  
Pour cela, assurez-vous que les colonnes du data.frame R sont au bon type.  
Si ce n'est pas le cas, utilisez les fonctions de conversion `as.character()`, `as.integer()`, `as.Date()` ...
  
<br>

```{.r}
#Créez un DataFrame R en spécifiant le type de chaque colonne
df <- data.frame( 
  colonne1 = as.integer(c(1, 2, 3)), 
  colonne2 = as.character(c("A", "B", "C")), 
  colonne3 = as.logical(c(TRUE, FALSE, TRUE)),
  colonne4 = as.numeric(35.43, 29.93, 17.02))

write_parquet(table, "output.parquet")
```

<br>
Une autre façon de forcer le typage des colonnes est d'utiliser un `schema`. Cependant, cela apporte de la complexité au code pas forcément utile.  
En cas de besoin, un exemple est disponible sur [cette page](https://gist.github.com/ddotta/d3900a8c7d2d247a90de6f8c6a261172).

## Ajouter des métadonnées générales sur un fichier Parquet (1/3)

<br>

A partir d'un data.frame R et avant l'écriture du fichier Parquet correspondant, 
il est possible d'ajouter des métadonnées générales sur un fichier Parquet.

<br>

Soit le data.frame suivant :  


```{.r}
df <- data.frame( 
  colonne1 = c(1, 2, 3), 
  colonne2 = c("A", "B", "C"), 
  colonne3 = c(TRUE, FALSE, TRUE),
  colonne4 = as.Date(c("2025-01-01", "2025-01-02", "2025-01-03")))
```

## Ajouter des métadonnées générales sur un fichier Parquet (2/3)

<br>

Avec uniquement le package {arrow}, on peut ajouter des métadonnées comme ceci :  

```{.r}
# Conversion en Arrow Table
df_arrow <- arrow_table(df)

# Ajouter des métadonnées
df_arrow <- df_arrow$ReplaceSchemaMetadata(c(
  auteur = "DEMESIS/BQIS",
  description = "Table test de formation",
  date_creation = as.character(Sys.Date())
))

write_parquet(df_arrow, "data/output.parquet")
```

## Ajouter des métadonnées générales sur un fichier Parquet (3/3)

<br>

Avec le package {nanoparquet}, le code est plus court.  
Attention ici, la fonction `write_parquet()` utilisée est issue du package {nanoparquet} 
et non {arrow} (qui ne propose pas l'argument `metadata` et dont les noms des autres arguments sont différents).  

<br>

```{.r}
nanoparquet::write_parquet(
  x = df,
  file = "data/output.parquet",
  metadata = c(
    auteur = "DEMESIS/BQIS",
    description = "Table test de formation",
    date_creation = as.character(Sys.Date()))
  )
```

## Industrialiser la conversion de vos fichiers ?

- Le package R [parquetize](https://ddotta.github.io/parquetize/) permet de faciliter la conversion de données au format Parquet.  

- Plusieurs formats supportés csv, json, rds, fst, SAS, SPSS, Stata, sqlite…  

- Propose des [solutions de contournement](https://ddotta.github.io/parquetize/articles/aa-conversions.html) pour les fichiers très volumineux.

Un exemple issu de la documentation :  

```{.r}
Conversion from a local rds file to a partitioned parquet file :: 12
rds_to_parquet(
path_to_file = system.file("extdata","iris.rds",package = "parquetize"),
path_to_parquet = tempfile(fileext = ".parquet"),
partition = "yes",
partitioning = c("Species")
)

#> Reading data...
#> Writing data...
#> ✔ Data are available in parquet dataset under /tmp/RtmptNiaDm/file1897441ca0c0.parquet
#> Writing data...

#> Reading data...
```

## Comment bien utiliser des fichiers partitionnés avec arrow (1/2)

<br>

La fonction `open_dataset()` permet **d'ouvrir une connexion vers un fichier Parquet partitionné**.  

L'utilisation de la fonction `open_dataset()` est **similaire au cas dans lequel on travaille avec un seul fichier Parquet**.

[Il y a toutefois 2 différences :]{.red}  

- Le chemin indiqué n'est pas celui d'un fichier .parquet, mais <u>le chemin d'un répertoire</u> dans lequel se trouve le
fichier Parquet partitionné
Il est préférable </u>d'indiquer le nom et le type de la ou des variable(s) de partitionnement</u>.

## Comment bien utiliser des fichiers partitionnés avec arrow (2/2)

<br>

Un exemple avec les données de la MSA :

```{.r}
# Établir la connexion au fichier Parquet partitionné
donnees_msa <- open_dataset(
  "data/msa_ns", # Ici, on met le chemin d'un répertoire
  hive_style = TRUE,
  partitioning = arrow::schema(TYPE_EXP = arrow::utf8(), SEXE = arrow::utf8()) # Les variables de   partitionnement
)

# Définir la requête
resultats_msa <- donnees_msa |>
  filter(TYPE_EXP == "2" & SEXE == "1") |> # Ici, on filtre selon les clés de partitionnement
  select(DEPT, RC_CHEF) |> 
  collect()
```

Ce qui donne : 

```{.r}
resultats_msa

> resultats_msa
# A tibble: 62,195 × 2
   DEPT  RC_CHEF
 * <chr> <chr>  
 1 11    1461   
 2 11    2910   
 3 11    1528   
 4 11    4493   
 5 11    1490   
# ℹ 62,185 more rows
# ℹ Use `print(n = ...)` to see more rows
```

## Conseils lors de l'utilisation de fichiers partitionnés

Afin de tirer au mieux profit du partitionnement, il est conseillé de **filtrer les données de préférence selon les variables de partitionnement** (dans notre exemple, `TYP_EXP` et `SEXE`).  

Il est fortement recommandé de **spécifier le type des variables de partitionnement avec l'argument
partitioning**.  

Cela évite des erreurs typiques: le code du département est interprété à tort comme un nombre et aboutit à une erreur à cause de la Corse...  

L'argument `partitioning` s'utilise en construisant un schéma qui précise le type de chacune des variables de partitionnement.   

Voir [cette page](https://arrow.apache.org/docs/r/reference/data-type.html) pour la liste des types supportés.

## Dernier conseil avec `arrow`

Il est recommandé de définir les deux options suivantes au début de votre script.  

Cela autorise arrow à utiliser plusieurs processeurs à la fois, ce qui accélère les traitements :  

```{.r}
Autoriser arrow à utiliser plusieurs processeurs en même temps
options(arrow.use_threads = TRUE)

# Définir le nombre de processeurs utilisés par arrow
# 10 processeurs sont suffisants dans la plupart des cas
arrow:::set_cpu_count(parallel::detectCores() %/% 2)
```