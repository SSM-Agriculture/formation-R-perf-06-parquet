---
editor: 
  markdown: 
    wrap: 72
---

# Comment utiliser/interroger un fichier parquet ? {.backgroundTitre}

## Lire un fichier avec read_parquet()

<br>

``` r
library(arrow)    # Le package arrow est nécessaire pour travailler avec des fichiers parquet
library(dplyr)    # Pour utiliser dplyr
library(tictoc)   # Pour le benchmark
```

<br>

Pour l'exemple, nous allons prendre une table des exploitations du RA
2020 d'une centaine de MO qui contient 416 478 lignes et 255 colonnes.

``` r
tic()
RA2020 <- arrow::read_parquet("data/RA2020_exploitations.parquet")
toc()
> 1.14 sec elapsed
```

Le résultat obtenu est un objet directement utilisable dans R (ici un
data.frame).

Il est possible de sélectionner les colonnes que l'on souhaite importer
dans R directement dans la fonction `read_parquet` :

``` r
tic()
RA2020_extrait <- arrow::read_parquet("data/RA2020_exploitations.parquet",
                                      col_select = c("NOM_DOSSIER","SIEGE_REG","SAU_TOT"))
toc()
> 0.06 sec elapsed
```

## Comparaison avec la lecture d'un fichier rds

<br>

Voyons l'écart avec la lecture d'un fichier rds :

``` r
tic()
RA2020 <- readRDS("data/RA2020_exploitations.rds")
toc()
> 6.15 sec elapsed
```

<br> <br>

=\> Le temps nécessaire au chargement de la table est d'environ 6
secondes ! <br> L'écart est significatif rien que sur la lecture (X 6).

## Des requêtes avec dplyr comme d'habitude

RA2020 est un data.frame : on peut donc utiliser la syntaxe dplyr :

``` r
resultat <- RA2020 |> 
  filter(SIEGE_REG == "93") |> 
  group_by(SIEGE_DEP) |> 
  summarise(total_SAU = sum(SAU_TOT, na.rm = TRUE))
  
# A tibble: 6 × 2
  SIEGE_DEP total_SAU
  <chr>         <dbl>
1 04          158946.
2 05           91979.
3 06           41141.
4 13          145713.
5 83           77785.
6 84          112888.
```

-   Le temps d'exécution de la requête est d'environ 9 secondes.
-   Les ressources consommées sont importantes

![](img/ressources_dplyr.png){fig-align="center"}

## Lire et exploiter un fichier parquet volumineux

<br>

Voici ci-dessous la syntaxe recommandée pour requêter un fichier parquet
volumineux :

<br>

``` r
# Établir la connexion aux données
RA2020 <- open_dataset("data/RA2020_exploitations.parquet")  |>
  filter(SIEGE_REG == "93") |> 
  group_by(SIEGE_DEP) |> 
  summarise(total_SAU = sum(SAU_TOT, na.rm = TRUE)) |> 
  collect()
```

=\> Avec cette syntaxe, la requête va automatiquement utiliser les
variables du fichier Parquet dont elle a besoin (en l’occurrence
SIEGE_REG, SIEGE_DEP et SAU_TOT) et minimiser l'occupation de la mémoire
vive.

<br>

<p style="text-align: center;">

[Revenons dans le détail sur cette syntaxe...]{.Macaron2}

</p>

## La fonction `open_dataset()` (1/4)

<br>

<p style="text-align: center;">

[Comme la fonction `read_parquet()`, la fonction `open_dataset()` permet
de lire des données stockées en format Parquet.]{.content-box-green}

</p>

Le résultat obtenu avec la fonction `open_dataset()` n'est plus un
**data.frame** mais un **Arrow Table** qui est une structure de données
spécifique.

<br>

``` r
RA2020 <- open_dataset("data/RA2020_exploitations.parquet")

class(RA2020)

> [1] "FileSystemDataset" "Dataset" "ArrowObject" "R6" 
```

## La fonction `open_dataset()` (2/4)

La fonction `open_dataset()` crée un objet qui apparaît dans Values.

![](img/vue_values.png){fig-align="center"}

L'affichage dans la console d'un Arrow Table affiche uniquement les
métadonnées.

``` r
RA2020

> FileSystemDataset with 1 Parquet file
NOM_DOSSIER: string
TYPE_QUESTIONNAIRE: string
SEUIL_IFS: string
CHAMP_GEO: string
COEF_F: double
NUMSTRATE: string
STRATE: string
SIEGENAT: string
SIEGE_CODE_COM: string
SIEGE_LIEUDIT: string
SIEGE_LIEUDIT_CODE_DOM: string
SIEGE_LIB_COM: string
...
```

## La fonction `open_dataset()` (3/4)

Pour afficher le contenu d'un **Arrow Table**, il faut d'abord le
convertir en data.frame avec la fonction `collect()`.

```{.r}
RA2020 <- RA2020 |> collect()

class(RA2020)

> [1] "data.frame"

# L'opération ci-dessus est à éviter pour des tables volumineuses, si besoin de visualiser la table, on préfèrera :

extrait_RA2020 <- RA2020 |> slice_head(n = 100) |> collect()
```

<br>

Toutefois **rien ne presse** car la grande différence entre manipuler un
data.frame et un Arrow Table tient **au moteur d'exécution** :

-   Si on manipule un data.frame avec la syntaxe de dplyr, alors c'est
    le **moteur d'exécution de dplyr** qui fait les calculs
    {{< fa person-biking >}}

-   Si on manipule un Arrow Table avec la syntaxe de dplyr, alors c'est
    le **moteur d'exécution d'arrow** (nommé **acero**) qui fait les
    calculs. Et le moteur d'exécution d'arrow est beaucoup plus efficace
    et rapide {{< fa jet-fighter >}}

## La fonction `open_dataset()` (4/4)

<br> Il est recommandé de privilégier la fonction `open_dataset()` à la
fonction `read_parquet()` pour au moins 2 raisons :

-   `open_dataset()` crée une connexion au fichier Parquet mais elle
    n'importe pas les données contenues dans ce fichier =\> **une
    consommation de RAM moins importante !**

-   `open_dataset()` peut se connecter à un fichier Parquet unique mais
    aussi à des **fichiers Parquets partitionnés** (voir plus loin)
    
## Consulter les métadonnées d'un fichier Parquet (1/3)

Pour obtenir des informations générales sur le fichier (par exemple titre, auteur, date...), 
il faut utiliser la fonction `read_parquet()` avec l'argument `as_data_frame = FALSE` pour 
pouvoir accéder aux métadonnées globales via `$schema$metadata`.  

```{.r}
# Lire le fichier Parquet en tant qu'Arrow Table avec read_parquet()
arrow_table <- read_parquet("output.parquet", as_data_frame = FALSE)

# Accéder aux métadonnées du fichier
arrow_table$schema$metadata
```

## Consulter les métadonnées d'un fichier Parquet (2/3)

Le package `{nanoparquet}` permet aussi d'obtenir d'autres infos facilement depuis des fichiers parquet :

```{.r}
library(nanoparquet)

parquet_info("data/RA2020_exploitations.parquet")

# > # A data frame: 1 × 7
  file_name                         num_cols num_rows num_row_groups file_size parquet_version created_by                     
  <chr>                                <int>    <dbl>          <int>     <dbl>           <int> <chr>                          
1 data/RA2020_exploitations.parquet      255   416478              1  39896331               2 parquet-cpp-arrow version 9.0.0
```

## Consulter les métadonnées d'un fichier Parquet (3/3)

- Les types des colonnes :  

```{.r}
nanoparquet::parquet_column_types("data/output.parquet")

# Renvoit :
# A data frame: 4 × 6
# file_name    name     type       r_type    repetition_type logical_type
# * <chr>        <chr>    <chr>      <chr>     <chr>           <I<list>>   
#   1 test.parquet colonne1 DOUBLE     double    REQUIRED        <NULL>      
#   2 test.parquet colonne2 BYTE_ARRAY character REQUIRED        <STRING>    
#   3 test.parquet colonne3 BOOLEAN    logical   REQUIRED        <NULL>      
#   4 test.parquet colonne4 INT32      Date      REQUIRED        <DATE>       
```

Attention, Parquet propose **2 niveaux de type** :  
- Le [bas niveau](https://parquet.apache.org/docs/file-format/types/)
- Le [logical type](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md)

La fonction `parquet_column_types()` retourne les types de bas niveau dans la colonne `type` et les types logiques dans la colonne `logical_type`.

- Pour aller plus loin sur les métadonnées :  

```{.r}
parquet_metadata("data/RA2020_exploitations.parquet")
```

=> On obtient des informations sur le schéma, les row_groups, les column chunks...

## L'évaluation/exécution différée (1/4)

<br>

Cela signifie qu'**arrow** se contente de mémoriser les instructions,
sans faire aucun calcul tant que l'utilisateur ne le demande pas
explicitement.

Il existe **2 fonctions pour déclencher l'évaluation d'un traitement
arrow** mais qui présente des différences :

-   `collect()` qui renvoie le résultat du traitement sous la forme d'un
    **data.frame/tibble**\
-   `compute()` qui renvoie le résultat du traitement sous la forme d'un
    **Arrow Table**.

**La grande différence entre manipuler un tibble et manipuler un Arrow
Table tient au** <u> moteur d'exécution </u> :

-   Avec un **data.frame/tibble** =\> <u>moteur d'exécution de
    dplyr</u>\
-   Avec un **Arrow Table** =\> <u>moteur d'exécution d'arrow (nommé
    acero)</u> plus efficace que celui de dplyr

[**Dans les traitements intermédiaires, on privilégiera la fonction
`compute()` pour pouvoir utiliser le plus possible le moteur
acero.**]{.red}

## L'évaluation/exécution différée (2/4)

``` r
SAU_DEP <- RA2020 |> 
  group_by(SIEGE_DEP) |> 
  summarise(total_SAU = sum(SAU_TOT, na.rm = TRUE))
class(SAU_DEP)
> [1] "arrow_dplyr_query"

resultats <- SAU_DEP |> 
  filter(SIEGE_DEP == "13") |> 
  collect()
> # A tibble: 1 × 2
  SIEGE_DEP total_SAU
  <chr>         <dbl>
1 13          145713.
```

Dans l'exemple ci-dessus, **la première étape ne réalise aucun calcul
par elle-même, car elle ne comprend ni collect() ni compute()**.
<u>L'objet `SAU_DEP` n'est pas une table et ne contient pas de
données</u>, il contient simplement une requête (query) décrivant les
opérations à mener sur la table du RA.

[arrow analyse la requête avant de l'exécuter, et optimise le traitement
pour minimiser le travail.]{.Macaron2}\
[Dans notre exemple, arrow repère que la requête ne porte en fait que
sur le département 13, et commence donc par filtrer les données sur le
département avant de sommer la SAU les équipements, de façon à ne
conserver que le minimum de données nécessaires et à ne réaliser que le
minimum de calculs.]{.small}

## L'évaluation/exécution différée (3/4)

<br>

[L'évaluation/exécution différée est très puissante mais **présente des
limites**.]{.red}

On serait tentés d'écrire un traitement entier *en mode lazy* (sans
aucun `compute()` ni `collect()` dans les étapes intermédiaires) et de
faire un unique `compute()` ou `collect()` tout à la fin du traitement
afin que toutes les opérations soient optimisées en une seule étape.

<br>

Malheureusement, **le moteur acero** a ses limites notamment sur des
traitements trop complexes (ce qui génère des plantages de sessions R).

## L'évaluation/exécution différée (4/4)

<br>

[**QUELQUES CONSEILS POUR ÉLABORER LA BONNE STRATÉGIE AVEC L'ÉVALUATION
DIFFÉRÉE :**]{.red}

<br>

-   Décomposer le traitement en plusieurs étapes puis exécuter chaque
    étape séparément (avec un `compute()`)\
-   Définir la bonne longueur des étapes intermédiaires en gardant en
    tête :
    -   D'avoir des étapes de traitement qui **ne dépassent pas 40
        lignes de code**
    -   Que le séquencement des étapes soit **cohérent** avec l'objet du
        traitement
    -   Plus les **données sont volumineuses** OU les **opérations
        unitaires sont complexes**, plus **les étapes de traitement
        doivent être courtes/prudentes**

## Quelques manques sur le moteur acero (1/2)

<br>

La liste des fonctions du *tidyverse* supportées par acero est
disponible [sur cette
page](https://arrow.apache.org/docs/dev/r/reference/acero.html).

Il y a (encore) quelques grands absents, notamment :

-   `pivot_wider()` et `pivot_longer()` n'ont pas d'équivalent avec
    **acero**.

-   **les empilements de plusieurs tables avec une seule fonction**
    (`bind_rows()` dans dplyr).\
    Avec des Arrow Tables, il faut appeler plusieurs fois ces fonctions
    (en l'occurence `union()`. Par exemple :

``` r
resultats <- table1 |>
  union(table2) |>
  union(table3) |>
  compute()
```

## Quelques manques sur le moteur acero (2/2)

<br>

-   les *window functions* (ajouter à une table des informations issues
    d'une agrégation par groupe) comme par exemple :

``` r
res <- RA2020 |>
  group_by(SIEGE_REG) |>
  mutate(total_SAU = sum(SAU_TOT)) |>
  collect()
  
> Error: window functions not currently supported in Arrow
Call collect() first to pull data into R.
```

Remarque : le code ci-dessus fonctionne par contre en remplaçant le
`mutate()` par un `summarise()`.

## Comment contourner le problème d'acero ? (1/3)

<br> **Plusieurs solutions existent :**

1.  Comme suggéré par R, renoncer à manipuler les données sous forme
    d'Arrow Table avec le moteur acero en passant par un `collect()` et
    poursuivre le traitement avec le moteur d'exécution de dplyr (avec
    des performances moins importantes).

<br>

2.  Étudier le message d'erreur renvoyé par R et chercher à réécrire
    d'une autre façon le traitement.

## Comment contourner le problème d'acero ? (2/3)

<br>

*Exemple pour le point 2 issu [d'utilitr](https://book.utilitr.org/) :*

``` r
resultats <- bpe_ens_2018_arrow |>
  group_by(DEP) |>
  summarise(
    nb_boulangeries  = sum(NB_EQUIP * (TYPEQU == "B203")),
    nb_poissonneries = sum(NB_EQUIP * (TYPEQU == "B206"))
  ) |>
  compute()
  
> ! NotImplemented: Function 'multiply_checked' has no kernel matching input types (double, bool); pulling data into R
```

<br>

L'erreur vient de l'opération sum(NB_EQUIP \* (TYPEQU == "B203")) :
**arrow** ne parvient pas à faire la multiplication entre NB_EQUIP (un
nombre réel) et (TYPEQU == "B203") (un booléen).

## Comment contourner le problème d'acero ? (3/3)

<br>

=\> La solution est très simple: **il suffit de convertir (TYPEQU ==
"B203") en nombre entier avec la fonction as.integer() qui est supportée
par acero**.

Le code suivant peut alors être entièrement exécuté par acero:

``` r
resultats <- bpe_ens_2018_arrow |>
  group_by(DEP) |>
  summarise(
    nb_boulangeries  = sum(NB_EQUIP * as.integer(TYPEQU == "B203")),
    nb_poissonneries = sum(NB_EQUIP * as.integer(TYPEQU == "B206"))
  ) |>
  compute()
```

## En conclusion sur le package arrow

<br>

[**Le package arrow présente 3 avantages majeurs :**]{.red}

-   **Performances élevées** : arrow est très efficace et très rapide
    pour la manipulation de données tabulaires (nettement plus
    performant que dplyr par exemple)

-   **Usage réduit des ressources** : arrow est conçu pour ne charger en
    mémoire que le minimum de données. Cela permet de réduire
    considérablement les besoins en mémoire, même lorsque les données
    sont volumineuses

-   **Facilité d'apprentissage grâce aux approches dplyr et SQL**: arrow
    peut être utilisé avec les verbes de dplyr (select, mutate, etc.)
    et/ou avec le langage SQL grâce à DuckDB (voir plus loin).

## Exercice 1

::: {.callout-tip icon="false"}
## Exercice 1 (premiers contacts avec un fichier parquet + rappels sur les fonctions)

-   Ouvrir le fichier parquet situé sous
    `~/CERISE/03-Espace-de-Diffusion/030_Structures_exploitations/3020_Recensements/RA_2020/01_BASES DIFFUSION RA2020/RA_2020_parquet/RA2020_EXPLOITATIONS_240112.parquet`

-   Consulter les métadonnées de ce fichier\

-   Consulter les 100 premières lignes de ce fichier\

-   Récupérer dans un vecteur trié les codes régions des lieux
    principaux de production (SIEGE_REG)\

-   Récupérer dans un vecteur trié les libellés régions des lieux
    principaux de production (SIEGE_LIB_REG)

-   Ecrire une fonction calculs_RA() qui - pour une région et une table
    donnée en entrée - conserve uniquement les lignes correspondantes
    selon la colonne SIEGE_REG, puis groupe la table par SIEGE_DEP et
    calcule la surface totale SAU (SAU_TOT), la surface totale de
    céréales (CEREALES_SUR_TOT) et la surface totale d'oléagineux
    (OLEAG_SUR_TOT) et enfin la part de la surface des céréales dans la
    SAU totale et la part de la surface des oléagineux dans la SAU
    totale.

-   Utiliser ensuite la fonction calculs_RA() pour calculer ces
    indicateurs sur l'ensemble des régions présentes dans la table du
    RA2020 et stocker les résultats dans des fichiers Excel sous votre
    espace personnel.\
    *TIPS : pensez à utiliser {purrr} et {openxlsx} par exemple.*
:::

## Exercice 2

::: {.callout-tip icon="false"}
## Exercice 2 (`collect()` vs `compute()`)

-   Dans votre espace de travail, créer les 2 fichiers parquet suivants
    :

```{.r}
data_a <- tibble(
  id = rep(1:1000000, each = 10),
  annee = rep(2016:2025, times = 1000000),
  a = sample(letters, 10000000, replace = TRUE)
)

data_b <- tibble(
  id = rep(1:1000000, each = 10),
  annee = rep(2016:2025, times = 1000000),
  b = runif(10000000, 1, 100)
)

data_c <- tibble(
  lettres = sample(letters, 10000000, replace = TRUE),
  classe = sample(c("pommes","poires","melon","fraise"), 10000000, replace = TRUE)
)

write_parquet(data_a, "data_a.parquet")
write_parquet(data_b, "data_b.parquet")
write_parquet(data_c, "data_c.parquet")

rm(data_a)
rm(data_b)
rm(data_c)
gc()
```

-   La suite de l'exercice sur la slide suivante...
:::

## Exercice 2 (suite)

::: {.callout-tip icon="false"}
## Exercice 2 (`collect()` vs `compute()`)

A)  AVEC `collect()`

Charger les fichiers parquet `data_a` et `data_b` sous forme de
data.frame\
Créer la table `etape1` en réalisant une jointure à gauche de `data_a`
avec `data_b`.\
Charger le fichier parquet `data_c` sous forme de data.frame\
Filtrer la table `etape1` sur les années supérieures à 2020 puis faire
la somme de la colonne `b` selon la colonne `a`\
Ajouter le colonne `classe` issue de la table `data_c` dans le tableau
final.

<br>

B)  AVEC `compute()`

Réaliser les mêmes traitements que A) avec des compute() et réduire le temps d'exécution.
:::
