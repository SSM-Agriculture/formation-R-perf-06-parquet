---
editor: 
  markdown: 
    wrap: 72
---

# Comment utiliser/interroger un fichier parquet ? {.backgroundTitre}

## Lire un fichier avec read_parquet()

<br>

```{.r}
library(arrow)    # Le package arrow est nécessaire pour travailler avec des fichiers parquet
library(dplyr)    # Pour utiliser dplyr
library(tictoc)   # Pour le benchmark
```
<br>

Pour l'exemple, nous allons prendre une table d'une centaine de MO qui
contient 518 925 lignes et 486 colonnes.

```{.r}
RA2010 <- arrow::read_parquet("data/RA2010_exploitations.parquet")
> 2.56 sec elapsed
```

Le résultat obtenu est un objet directement utilisable dans R (ici un data.frame).  

Il est possible de sélectionner les colonnes que l'on souhaite importer dans R directement dans la fonction `read_parquet` :  

<br>

```{.r}
RA2010_extrait <- arrow::read_parquet("data/RA2010_exploitations.parquet",
                                      col_select = c("ID_DOSSIER","region","SAU"))
> 0.15 sec elapsed
```


## Comparaison avec la lecture d'un fichier rds

<br>

Voyons l'écart avec la lecture d'un fichier rds :

```{.r}
RA2010 <- readRDS("data/RA2010_exploitations.rds")
> 31.83 sec elapsed
```

<br>
<br>

- Le temps nécessaire au chargement de la table est d'environ 32 secondes
!
<br>
- L'écart est significatif rien que sur la lecture (X 13).  


## Des requêtes avec dplyr comme d'habitude

RA2010 est un data.frame : on peut donc utiliser la syntaxe dplyr :

```{.r}
resultat <- RA2010 |> 
  filter(region == "93") |> 
  group_by(dep) |> 
  summarise(total_SAU = sum(SAU, na.rm = TRUE))
  
# A tibble: 6 × 2
  dep   total_SAU
  <chr>     <dbl>
1 04     19348882
2 05     21368858
3 06      5564296
4 13     16531479
5 83      7387106
6 84     11344215
```

-   Le temps d'exécution de la requête est d'environ 9 secondes.
-   Les ressources consommées sont importantes

![](img/ressources_dplyr.png){fig-align="center"}

## Lire et exploiter un fichier parquet volumineux

<br>

Exemple avec une table volumineuse (Recensements Insee 1968-2019, soit 51 millions de lignes et 18 colonnes), suivre [ce lien](https://gist.github.com/ddotta/acf6add0f2328f077791461ef4f37b84) pour obtenir le code qui permet de générer “Ficdep19.parquet” de façon reproductible) :

<br>

```{.r}
# Établir la connexion aux données
donnees_Ficdep19 <- open_dataset("data/Ficdep19.parquet")  |>
  filter(DEP_RES_21 == "11") |>
  group_by(SEXE) |>
  summarise(total = sum(pond)) |>
  collect()
```

=> Avec cette syntaxe, la requête va automatiquement utiliser les variables du fichier Parquet dont elle a besoin (en
l’occurence DEP_RES_21, SEXE et pond) et minimiser l’occupation de la mémoire vive.

<br>

<p style="text-align: center;">[Revenons dans le détail sur cette syntaxe...]{.Macaron2}</p>

## La fonction `open_dataset()` (1/4)

<br>

<p style="text-align: center;">[Comme la fonction `read_parquet()`, la fonction `open_dataset()` permet de lire des données stockées en format Parquet. ]{.content-box-green}</p>

 
Le résultat obtenu avec la fonction `open_dataset()` n'est plus un **data.frame** mais un **Arrow Table** qui est une structure de données spécifique.

<br>

```{.r}
RA2010 <- open_dataset("data/RA2010_exploitations.parquet")

class(RA2010)

> [1] "FileSystemDataset" "Dataset" "ArrowObject" "R6" 
```

## La fonction `open_dataset()` (2/4)

La fonction `open_dataset()` crée un objet qui apparaît dans Values. 

![](img/vue_values.png){fig-align="center"}

L’affichage dans la console d’un Arrow Table affiche uniquement des métadonnées.

```{.r}
RA2010

> FileSystemDataset with 1 Parquet file
ID_DOSSIER: string
IDENTIFIANT: string
DEVINCERTAIN: string
COLLECTIFFIL: string
VACANTE: string
SIEGEREGDEP: string
SIEGEDEPCOM: string
region: string
dep: string
CHAMP2: string
CHAMP3: string
pbstot_07e: double
...
```


## La fonction `open_dataset()` (3/4)

Pour afficher le contenu d’un **Arrow Table**, il faut d’abord le convertir en tibble avec la fonction `collect()`. 


```{.r}
RA2010 <- RA2010 |> collect()

class(RA2010)

> [1] "data.frame"
```

<br>

Toutefois **rien ne presse** car la grande différence entre manipuler un data.frame et un Arrow Table tient **au moteur d’exécution** : 

- Si on manipule un data.frame avec la syntaxe de dplyr, alors c’est le **moteur d’exécution de dplyr** qui fait les calculs {{< fa person-biking >}}

- Si on manipule un Arrow Table avec la syntaxe de dplyr, alors c’est le **moteur d’exécution d’arrow** (nommé **acero**) qui fait les calculs. Et le moteur d’exécution d’arrow est beaucoup plus efficace et rapide {{< fa jet-fighter >}}

## La fonction `open_dataset()` (4/4)

<br>
Il est recommandé de privilégier la fonction `open_dataset()` à la fonction `read_parquet()` pour au moins 2 raisons :  

- `open_dataset()` crée une connexion au fichier Parquet mais elle n'importe pas les données contenues dans ce fichier => **une consommation de RAM moins importante !**

- `open_dataset()` peut se connecter à un fichier Parquet unique mais aussi à des **fichiers Parquets partitionnés** (voir plus loin)

## L'éxécution différée

<br> 

Cela signifie qu’**arrow** se contente de mémoriser les instructions, sans faire aucun calcul tant que l’utilisateur ne le demande pas explicitement.  

Il existe **2 fonctions pour déclencher l’évaluation d’un traitement arrow**: 

- `collect()`  
- `compute()`.

<br>
Quelle différence ?  

- `collect()` renvoie le résultat du traitement sous la forme d’un **data.frame**  
- `compute()`  renvoie le résultat du traitement sous la forme d’un **Arrow Table**

::: callout-important
Dans les traitements intermédiaires, on privilégiera la fonction `compute()` pour pouvoir utiliser le plus possible le moteur acero.
:::

## En conclusion sur le package arrow

[Le package arrow présente 3 avantages majeurs :]{.red}

- **Performances élevées** : arrow est très efficace et très rapide pour la manipulation de données tabulaires (nettement plus performant que dplyr par exemple)  

- **Usage réduit des ressources** : arrow est conçu pour ne charger en mémoire que le minimum de données. Cela permet de réduire considérablement les besoins en mémoire, même lorsque les données sont volumineuses  

- **Facilité d’apprentissage grâce aux approches dplyr et SQL**: arrow peut être utilisé avec les verbes de dplyr (select, mutate, etc.) et/ou avec le langage SQL grâce à DuckDB.





