---
editor: 
  markdown: 
    wrap: 72
---

# Comment utiliser/interroger un fichier parquet ? {.backgroundTitre}

## Lire un fichier avec read_parquet()

<br>

```{.r}
library(arrow)    # Le package arrow est nécessaire pour travailler avec des fichiers parquet
library(dplyr)    # Pour utiliser dplyr
library(tictoc)   # Pour le benchmark
```
<br>

Pour l'exemple, nous allons prendre une table des eploitations du RA 2020 d'une centaine de MO qui
contient 416 478 lignes et 255 colonnes.

```{.r}
tic()
RA2020 <- arrow::read_parquet("data/RA2020_exploitations.parquet")
toc()
> 1.14 sec elapsed
```

Le résultat obtenu est un objet directement utilisable dans R (ici un data.frame).  

Il est possible de sélectionner les colonnes que l'on souhaite importer dans R directement dans la fonction `read_parquet` :  

<br>

```{.r}
tic()
RA2020_extrait <- arrow::read_parquet("data/RA2020_exploitations.parquet",
                                      col_select = c("NOM_DOSSIER","SIEGE_REG","SAU_TOT"))
toc()
> 0.06 sec elapsed
```


## Comparaison avec la lecture d'un fichier rds

<br>

Voyons l'écart avec la lecture d'un fichier rds :

```{.r}
tic()
RA2020 <- readRDS("data/RA2020_exploitations.rds")
toc()
> 6.15 sec elapsed
```

<br>
<br>

- Le temps nécessaire au chargement de la table est d'environ 6 secondes
!
<br>
- L'écart est significatif rien que sur la lecture (X 6).  


## Des requêtes avec dplyr comme d'habitude

RA2020 est un data.frame : on peut donc utiliser la syntaxe dplyr :

```{.r}
resultat <- RA2020 |> 
  filter(SIEGE_REG == "93") |> 
  group_by(SIEGE_DEP) |> 
  summarise(total_SAU = sum(SAU_TOT, na.rm = TRUE))
  
# A tibble: 6 × 2
  SIEGE_DEP total_SAU
  <chr>         <dbl>
1 04          158946.
2 05           91979.
3 06           41141.
4 13          145713.
5 83           77785.
6 84          112888.
```

-   Le temps d'exécution de la requête est d'environ 9 secondes.
-   Les ressources consommées sont importantes

![](img/ressources_dplyr.png){fig-align="center"}

## Lire et exploiter un fichier parquet volumineux

<br>

Exemple avec une table volumineuse (Recensements Insee 1968-2019, soit 51 millions de lignes et 18 colonnes), suivre [ce lien](https://gist.github.com/ddotta/acf6add0f2328f077791461ef4f37b84) pour obtenir le code qui permet de générer “Ficdep19.parquet” de façon reproductible) :

<br>

```{.r}
# Établir la connexion aux données
donnees_Ficdep19 <- open_dataset("data/RA2020_exploitations.parquet")  |>
  filter(region == "93") |> 
  group_by(dep) |> 
  summarise(total_SAU = sum(SAU, na.rm = TRUE)) |>
  collect()
```

=> Avec cette syntaxe, la requête va automatiquement utiliser les variables du fichier Parquet dont elle a besoin (en
l’occurence region, dep et SAU) et minimiser l’occupation de la mémoire vive.

<br>

<p style="text-align: center;">[Revenons dans le détail sur cette syntaxe...]{.Macaron2}</p>

## La fonction `open_dataset()` (1/4)

<br>

<p style="text-align: center;">[Comme la fonction `read_parquet()`, la fonction `open_dataset()` permet de lire des données stockées en format Parquet. ]{.content-box-green}</p>

 
Le résultat obtenu avec la fonction `open_dataset()` n'est plus un **data.frame** mais un **Arrow Table** qui est une structure de données spécifique.

<br>

```{.r}
RA2020 <- open_dataset("data/RA2020_exploitations.parquet")

class(RA2020)

> [1] "FileSystemDataset" "Dataset" "ArrowObject" "R6" 
```

## La fonction `open_dataset()` (2/4)

La fonction `open_dataset()` crée un objet qui apparaît dans Values. 

![](img/vue_values.png){fig-align="center"}

L’affichage dans la console d’un Arrow Table affiche uniquement des métadonnées.

```{.r}
RA2020

> FileSystemDataset with 1 Parquet file
NOM_DOSSIER: string
TYPE_QUESTIONNAIRE: string
SEUIL_IFS: string
CHAMP_GEO: string
COEF_F: double
NUMSTRATE: string
STRATE: string
SIEGENAT: string
SIEGE_CODE_COM: string
SIEGE_LIEUDIT: string
SIEGE_LIEUDIT_CODE_DOM: string
SIEGE_LIB_COM: string
...
```


## La fonction `open_dataset()` (3/4)

Pour afficher le contenu d’un **Arrow Table**, il faut d’abord le convertir en tibble avec la fonction `collect()`. 


```{.r}
RA2020 <- RA2020 |> collect()

class(RA2020)

> [1] "data.frame"

# L'opération ci-dessus est à éviter pour des tables volumineuses, si besoin de visualiser la table, on préfèrera :

extrait_RA2020 <- RA2020 |> slice_head(n = 100) |> collect()
```

<br>

Toutefois **rien ne presse** car la grande différence entre manipuler un data.frame et un Arrow Table tient **au moteur d’exécution** : 

- Si on manipule un data.frame avec la syntaxe de dplyr, alors c’est le **moteur d’exécution de dplyr** qui fait les calculs {{< fa person-biking >}}

- Si on manipule un Arrow Table avec la syntaxe de dplyr, alors c’est le **moteur d’exécution d’arrow** (nommé **acero**) qui fait les calculs. Et le moteur d’exécution d’arrow est beaucoup plus efficace et rapide {{< fa jet-fighter >}}

## La fonction `open_dataset()` (4/4)

<br>
Il est recommandé de privilégier la fonction `open_dataset()` à la fonction `read_parquet()` pour au moins 2 raisons :  

- `open_dataset()` crée une connexion au fichier Parquet mais elle n'importe pas les données contenues dans ce fichier => **une consommation de RAM moins importante !**

- `open_dataset()` peut se connecter à un fichier Parquet unique mais aussi à des **fichiers Parquets partitionnés** (voir plus loin)

## L'évalutation/éxécution différée (1/4)

<br> 

Cela signifie qu’**arrow** se contente de mémoriser les instructions, sans faire aucun calcul tant que l’utilisateur ne le demande pas explicitement.  

Il existe **2 fonctions pour déclencher l’évaluation d’un traitement arrow**: 

- `collect()`  
- `compute()`.

<br>
Quelle différence ?  

- `collect()` renvoie le résultat du traitement sous la forme d’un **data.frame/tibble**  
- `compute()`  renvoie le résultat du traitement sous la forme d’un **Arrow Table**

::: callout-important
Dans les traitements intermédiaires, on privilégiera la fonction `compute()` pour pouvoir utiliser le plus possible le moteur acero.
:::

## L'évalutation/éxécution différée (2/4)

<br>

Prenons l'exemple pédagogique suivant :

```{.r}
SAU_DEP <- RA2020 |> 
  group_by(SIEGE_DEP) |> 
  summarise(total_SAU = sum(SAU_TOT, na.rm = TRUE))

class(SAU_DEP)
> [1] "arrow_dplyr_query"

resultats <- SAU_DEP |> 
  filter(SIEGE_DEP == "13") |> 
  collect()
  
> # A tibble: 1 × 2
  SIEGE_DEP total_SAU
  <chr>         <dbl>
1 13          145713.
```


Il est important de comprendre que **la première étape ne réalise aucun calcul par elle-même, car elle ne comprend ni collect() ni compute()**. L’objet `SAU_DEP` n’est pas une table et ne contient pas de données, il contient simplement une requête (query) décrivant les opérations à mener sur la table du RA.

[arrow analyse la requête avant de l’exécuter, et optimise le traitement pour minimiser le travail]{.Macaron2}.  
Dans notre exemple, arrow repère que la requête ne porte en fait que sur le département 13, et commence donc par filtrer les données sur le département avant de sommer la SAU les équipements, de façon à ne conserver que le minimum de données nécessaires et à ne réaliser que le minimum de calculs. 

## L'évalutation/éxécution différée (3/4)

<br>

[L'évaluation/exécution différée est très puissante mais **présente des limites**.]{.red}  

On serait tentés d'écrire un traitement entier *en mode lazy* (sans aucun `compute()` ni `collect()` dans les étapes intermédiaires) et de faire un unique `compute()` ou `collect()` tout à la fin du traitement afin que toutes les opérations soient optimisées en une seule étape.  

<br>

Malheureusement, **le moteur acero** a ses limites notamment sur des traitements trop complexes (ce qui génère des plantages de sessions R).

## L'évalutation/éxécution différée (4/4)

<br>

[**QUELQUES CONSEILS POUR ÉLABORER LA BONNE STRATÉGIE AVEC L'ÉVALUATION DIFFÉRÉE :**]{.red}    

<br>

- Décomposer le traitement en plusieurs étapes puis exécuter chaque étape séparément (avec un `compute()`)  
- Définir la bonne longueur des étapes intermédiaires en gardant en tête :  
    - D'avoir des étapes de traitement qui **ne dépassent pas 40 lignes de code**
    - Que le séquencement des étapes soit **cohérent** avec l'objet du traitement
    - Plus les **données sont volumineuses** OU les **opérations unitaires sont complexes**, plus **les étapes de traitement doivent être courtes/prudentes**

## Notions avancées sur le moteur acero

La liste des fonctions du *tidyverse* supportées par acero est disponible [sur cette page](https://arrow.apache.org/docs/dev/r/reference/acero.html).  

Il y a (encore) quelques grands absents, notamment :  

- `pivot_wider()` et `pivot_longer()` n'ont pas d'équivalent avec **acero**. 

- **les empilements de plusieurs tables avec une seule fonction** (`bind_rows()` dans dplyr).  
Avec des Arrow Tables, il faut appeler plusieurs foirs ces fonctions (en l'occurence `union()`. Par exemple :  

```{.r}
resultats <- table1 |>
  union(table2) |>
  union(table3) |>
  compute()
```

- les *window functions* (ajouter à une table des informations issues d'une agrégation par groupe) comme par exemple :  

```{.r}
res <- RA2020 |>
  group_by(SIEGE_REG) |>
  mutate(total_SAU = sum(SAU_TOT)) |>
  collect()
  
> Error: window functions not currently supported in Arrow
Call collect() first to pull data into R.
```

## Comment contourner le problème d'acero ?

<br>
**Plusieurs solutions existent :**  

1. Comme suggéré par R, renoncer à manipuler les données sous forme d'**Arrow Table** avec le moteur acero en passant par un `collect()` et poursuivre le traitement avec le moteur d'exécution de dplyr.  


2. Étudier le message d’erreur renvoyé par R et chercher à réécrire d'une autre façon le traitement.  

*Exemple pour le point 2 issu [d'utilitr](https://book.utilitr.org/) :*  

```{.r}
resultats <- bpe_ens_2018_arrow |>
  group_by(DEP) |>
  summarise(
    nb_boulangeries  = sum(NB_EQUIP * (TYPEQU == "B203")),
    nb_poissonneries = sum(NB_EQUIP * (TYPEQU == "B206"))
  ) |>
  compute()
  
> ! NotImplemented: Function 'multiply_checked' has no kernel matching input types (double, bool); pulling data into R
```

L’erreur vient de l’opération sum(NB_EQUIP * (TYPEQU == "B203")) : **arrow** ne parvient pas à faire la multiplication entre NB_EQUIP (un nombre réel) et (TYPEQU == "B203") (un booléen).  
=> La solution est très simple: **il suffit de convertir (TYPEQU == "B203") en nombre entier avec la fonction as.integer() qui est supportée par acero**. Le code suivant peut alors être entièrement exécuté par acero:

```{.r}
resultats <- bpe_ens_2018_arrow |>
  group_by(DEP) |>
  summarise(
    nb_boulangeries  = sum(NB_EQUIP * as.integer(TYPEQU == "B203")),
    nb_poissonneries = sum(NB_EQUIP * as.integer(TYPEQU == "B206"))
  ) |>
  compute()
```


## En conclusion sur le package arrow

<br>

[**Le package arrow présente 3 avantages majeurs :**]{.red}

- **Performances élevées** : arrow est très efficace et très rapide pour la manipulation de données tabulaires (nettement plus performant que dplyr par exemple)  

- **Usage réduit des ressources** : arrow est conçu pour ne charger en mémoire que le minimum de données. Cela permet de réduire considérablement les besoins en mémoire, même lorsque les données sont volumineuses  

- **Facilité d’apprentissage grâce aux approches dplyr et SQL**: arrow peut être utilisé avec les verbes de dplyr (select, mutate, etc.) et/ou avec le langage SQL grâce à DuckDB.





